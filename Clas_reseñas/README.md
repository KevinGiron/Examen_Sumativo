# Clasificador de Opiniones/ReseÃ±as mediante Machine Learning

Sistema completo de clasificaciÃ³n automÃ¡tica de reseÃ±as de clientes en categorÃ­as usando embeddings y modelos de ML.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un pipeline end-to-end para clasificar reseÃ±as de clientes en tres categorÃ­as:
- **Positivo**: ReseÃ±as satisfechas y recomendaciones
- **Negativo**: Quejas y experiencias negativas
- **Neutral**: ReseÃ±as con opiniones mixtas o descriptivas

## ğŸ—ï¸ Arquitectura del Pipeline

```
ReseÃ±as (texto)
    â†“
[Embeddings] - SentenceTransformer (384 dimensiones)
    â†“
[Entrenamiento] - 3 modelos (Logistic, SVM, Random Forest)
    â†“
[ValidaciÃ³n Cruzada] - 5-fold cross-validation
    â†“
[EvaluaciÃ³n] - Accuracy, F1, Precision, Recall
    â†“
[Despliegue] - Predicciones en nuevas reseÃ±as
```

## ğŸ“ Estructura del Proyecto

```
Clas_reseÃ±as/
â”œâ”€â”€ main.py                      # Script principal de ejecuciÃ³n
â”œâ”€â”€ requirements.txt             # Dependencias del proyecto
â”œâ”€â”€ README.md                    # Este archivo
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ reviews.csv         # Dataset de 60+ reseÃ±as etiquetadas
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sentiment_model_logistic.pkl     # Modelo de regresiÃ³n logÃ­stica
â”‚   â”œâ”€â”€ sentiment_model_svm.pkl          # Modelo SVM
â”‚   â””â”€â”€ sentiment_model_random_forest.pkl # Modelo Random Forest
â””â”€â”€ src/
    â”œâ”€â”€ data_loader.py           # Carga de datos desde CSV o carpetas
    â”œâ”€â”€ embeddings.py            # GeneraciÃ³n de embeddings con ST
    â”œâ”€â”€ train.py                 # Entrenamiento con mÃºltiples modelos
    â”œâ”€â”€ evaluate.py              # EvaluaciÃ³n exhaustiva y anÃ¡lisis
    â””â”€â”€ predict.py               # Predicciones en nuevos textos
```

## ğŸš€ ConfiguraciÃ³n Inicial

### 1. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 2. Dataset

El dataset se proporciona en `data/raw/reviews.csv` con formato:
```csv
text,label
"Texto de reseÃ±a",positivo
"Otra reseÃ±a",negativo
...
```

### 3. Ejecutar Pipeline Completo

```bash
python main.py
```

## ğŸ”§ Componentes

### 1. **Data Loader** (`src/data_loader.py`)

**Funciones:**
- `load_csv_data(csv_path)`: Carga reseÃ±as desde CSV
- `load_imdb_data(data_dir)`: Carga desde estructura de carpetas

**Salida:** Textos y etiquetas

### 2. **Embeddings** (`src/embeddings.py`)

**Modelo:** `all-MiniLM-L6-v2` de SentenceTransformers
- Dimensionalidad: 384
- Tiempo: ~0.5ms por reseÃ±a
- Optimizado para frases cortas

```python
embedder = Embedder()
embeddings = embedder.encode(texts)  # shape: (N, 384)
```

### 3. **Entrenamiento** (`src/train.py`)

**Modelos Disponibles:**
- **Logistic Regression**: RÃ¡pido, interpretable, baseline
- **Linear SVM**: Bueno para espacios de alta dimensiÃ³n
- **Random Forest**: No-lineal, robusto

**CaracterÃ­sticas:**
- EstratificaciÃ³n en train/test split
- ValidaciÃ³n cruzada (5-fold)
- MÃ©tricas detalladas por clase

```python
from src.train import train_model, save_model

clf, results = train_model(
    embeddings, labels,
    model_type="logistic",
    test_size=0.2,
    cv_folds=5
)
```

### 4. **EvaluaciÃ³n** (`src/evaluate.py`)

**MÃ©tricas:**
- Accuracy, Precision, Recall, F1-Score
- Matriz de confusiÃ³n
- Reporte por clase
- AnÃ¡lisis de readiness para despliegue

```python
from src.evaluate import evaluate_model, assess_deployment_readiness

metrics = evaluate_model(y_true, y_pred)
assessment = assess_deployment_readiness(metrics, min_f1=0.80)
```

### 5. **PredicciÃ³n** (`src/predict.py`)

```python
from src.predict import predict, predict_batch

# Una texto
prediction, confidence = predict("Excelente producto")

# MÃºltiples textos
predictions, confidences = predict_batch(texts_list)
```

## ğŸ“Š Resultados Esperados

Con el dataset de 60+ reseÃ±as balanceadas:

| Modelo | Accuracy | F1-Score | Comentario |
|--------|----------|----------|-----------|
| Logistic Regression | ~0.78-0.82 | ~0.77-0.81 | Baseline rÃ¡pido |
| Linear SVM | ~0.80-0.85 | ~0.79-0.84 | SÃ³lido en embeddings |
| Random Forest | ~0.75-0.80 | ~0.74-0.79 | Puede overfitear con dataset pequeÃ±o |

## ğŸ¯ Readiness para Despliegue

### Criterios de AceptaciÃ³n
- âœ… **F1-Score â‰¥ 0.80**: Considerado production-ready
- âœ… **Balanced performance**: F1 similar en todas las clases
- âœ… **ValidaciÃ³n cruzada estable**: CV std < 0.05

### Criterios Actual (Dataset pequeÃ±o)
- âœ… **F1-Score â‰¥ 0.65**: Aceptable para MVP
- âš ï¸ **MÃ¡s datos recomendado**: +1000 reseÃ±as para mejor performance
- âš ï¸ **Monitoreo en producciÃ³n**: Esencial

## ğŸ“ˆ Recomendaciones para Mejora

### Corto Plazo (Semanas)
1. **Expandir dataset** a 500-1000 reseÃ±as
2. **Fine-tuning** de embeddings para espaÃ±ol
3. **Grid search** de hiperparÃ¡metros

### Mediano Plazo (Meses)
1. **Usar BERT especÃ­fico para espaÃ±ol** (beto, xlm-roberta)
2. **Ensamble de modelos** para mejor robustez
3. **Feedback loop** automÃ¡tico en producciÃ³n

### Largo Plazo (ProducciÃ³n)
1. **Reentrenamiento automÃ¡tico** con nuevos datos
2. **A/B testing** de versiones de modelo
3. **Monitoring** de performance y distribution shift
4. **Explicabilidad** (SHAP, LIME) para predicciones

## ğŸ” AnÃ¡lisis de Errores

**Causas tÃ­picas de falsos negativos:**
- Sarcasmo o ironia no detectado
- Contexto cultural no capturado
- Textos muy cortos o ambiguos

**Soluciones:**
- Aumentar datos de entrenamiento
- Manual review de misclassifications
- Ajustar thresholds de confianza

## ğŸ“ Ejemplo de Uso en ProducciÃ³n

```python
from src.predict import predict_batch
from src.embeddings import Embedder

# Nuevas reseÃ±as a clasificar
nuevas_resenas = [
    "Producto fantÃ¡stico, muy recomendado",
    "No funciona, pÃ©sima calidad",
    "Es un producto normal, nada especial"
]

# Predicciones
predictions, confidences = predict_batch(
    nuevas_resenas, 
    model_path="models/sentiment_model_logistic.pkl"
)

# Procesar resultados
for text, pred, conf in zip(nuevas_resenas, predictions, confidences):
    if conf < 0.6:
        print(f"REVISAR MANUALMENTE: '{text}' (confianza: {conf:.2%})")
    else:
        print(f"Clasificado como {pred} (confianza: {conf:.2%})")
```

## ğŸ§ª Testing

Para probar el pipeline:

```bash
# Entrenar modelos
python main.py

# Hacer predicciones en nueva reseÃ±a (agregar script predict_custom.py)
python src/predict.py "Mi texto a clasificar"
```

## ğŸ“š Dependencias

- `pandas`: ManipulaciÃ³n de datos
- `numpy`: CÃ¡lculos numÃ©ricos
- `scikit-learn`: Modelos ML y mÃ©tricas
- `sentence-transformers`: Embeddings (SentenceTransformer)
- `torch`: Backend de transformers
- `joblib`: SerializaciÃ³n de modelos
- `matplotlib`/`seaborn`: VisualizaciÃ³n (opcional)

## ğŸ“ Conceptos Clave

### Embeddings
RepresentaciÃ³n vectorial densa del texto que captura semÃ¡ntica. `SentenceTransformer` produce embeddings de 384 dimensiones optimizados para similaridad semÃ¡ntica.

### ValidaciÃ³n Cruzada
TÃ©cnica para evaluar modelo sin depender de un split aleatorio. K-fold divide datos en K subconjuntos, entrena K veces, promedia resultados.

### F1-Score
MÃ©trica que balancea precision y recall: `F1 = 2 * (precision * recall) / (precision + recall)`

Mejor que accuracy cuando hay desbalance de clases.

## âš–ï¸ Limitaciones Actuales

1. **Dataset pequeÃ±o**: 60 reseÃ±as (ideal: 1000+)
2. **Solo castellano**: Modelos optimizados para este idioma
3. **Textos relativamente cortos**: Rendimiento puede variar en textos muy largos
4. **Sin context**: No considera contexto histÃ³rico del cliente

## ğŸ“ Soporte y Mejoras

Para issues o sugerencias:
1. Revisar evaluaciÃ³n.py para mÃ©tricas detalladas
2. Analizar confusion matrix para patrones de error
3. Aumentar dataset con casos problemÃ¡ticos
4. Experimentar con diferentes modelos de embeddings

---

**VersiÃ³n**: 1.0  
**Ãšltima actualizaciÃ³n**: Febrero 2026  
**Estado**: Production-ready para MVP
